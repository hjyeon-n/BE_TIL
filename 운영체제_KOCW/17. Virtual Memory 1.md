# Virtual Memory 1

프로그램이 실행되기 위해서는 메모리에 올라와야 한다. 하지만 이전에 배웠듯이 모든 주소 공간을 메모리에 올릴 필요는 없다. 필요한 부분만 올려두고 나머지는 디스크의 스왑 영역에 내려놓았다가 필요해지면 메모리에 올려져 있는 부분과 교체하는 방식을 사용한다.

이 방식을 사용함으로써 프로그램 입장에서는 물리적 메모리 크기에 대한 제약을 생각할 필요가 없어지고, 나아가 운영체제는 프로그램이 물리적 메모리를 고려할 필요 없이 자기 자신만이 메모리를 사용하는 것처럼 가정할 수 있다. 이와 같은 메모리 공간을 **가상 메모리**(Virtual Memory)라고 한다.

<br>

### 요구 페이징

가상메모리 기법은 요구 페이징(demand paging) 방식과 요구 세그먼테이션(demand segmentation) 방식으로 구현될 수 있으나, 대부분의 경우 요구 페이징 방식을 사용하기 때문에 이 방식 위주로 설명하겠다! 😎

요구 페이징이란 프로그램 실행 시 프로세스를 구성하는 모든 페이지를 한꺼번에 메모리에 올리는 것이 아니라 당장 사용될 페이지만을 올리는 방식이다. 이 방식의 장점은 아래와 같다.

+ 당장 필요한 페이지만 올리기 때문에 **메모리 사용량 감소**
+ **입출력 오버헤드 감소**
+ 사용되지 않을 영역에 대한 입출력까지 수행하는 기존 방식에 비해 **응답시간 단축**
+ 시스템이 **더 많은 프로세스를 수용**할 수 있게 해줌

<br>

![image](https://user-images.githubusercontent.com/62419307/116114129-0c170b80-a6f4-11eb-8a43-cdb17a0ac0ed.png)

[이미지 출처](https://getchan.github.io/cs/OS_7/)

일부 페이지는 메모리에 있고 일부는 스왑 영역에 있기 때문에 어떤 페이지가 메모리에 존재하는지 안 하는지 구분하기 위해 **유효-무효 비트**(valid-invalid bit)을 둔다. 이 비트는 모든 페이지에 대해 존재해야 하기 때문에 페이지 테이블의 각 항목별로 저장된다.

+ 메모리에 적재되는 경우 유효값
+ 페이지가 현재 메모리에 없는 경우 또는 그 페이지가 속한 주소 영역을 프로세스가 사용하지 않는 경우

CPU가 참조하려는 페이지가 무효 비트일 경우엔 **페이지 부재**(page fault)라고 한다.

<br>

#### 요구 페이징의 페이지 부재 처리

![image](https://user-images.githubusercontent.com/62419307/116114289-2f41bb00-a6f4-11eb-9ae4-c5a92d22b5d8.png)

[이미지 출처](https://getchan.github.io/cs/OS_7/)

1. CPU가 무효 페이지에 접근하면 MMU가 페이지 부재 트랩을 발생시킨다.
2. 운영체제의 페이지 부재 처리루틴이 호출되어 해당 페이지에 대한 접근의 적법성을 검사한다. 만약 사용되지 않는 주소 영역에 속한 페이지에 접근하거나 접근 권한 위반을 했을 경우에는 해당 프로세스를 종료시킨다.
3. 적법한 경우 물리적 메모리에서 비어있는 프레임을 할당받아 그 공간의 페이지를 읽어온다.
4. 만약 비어있는 프레임이 없다면 스왑아웃 시킨다. 디스크에 접근해야 하기 때문에 해당 프로세스는 봉쇄 상태가 된다.
5. 디스크 입출력이 완료되면 페이지 테이블에서 해당 페이지의 유효-무효 비트를 유효 비트로 설정하고 준비 큐로 이동시킨다.
6. CPU를 할당 받으면 중단되었던 명령부터 실행을 재개한다.

<br>

#### 요구 페이징의 성능

요구 페이징 성능은 페이지 부재의 발생빈도에 가장 큰 영향을 받는다. 디스크로부터 메모리로 읽어오는 막대한 오버헤드가 발생하기 때문이다. 따라서 페이지 부재가 적게 발생할수록 요구 페이징의 성능은 향상될 수 있다.

<br>

## 페이지 교체

물리적 메모리에 빈 프레임이 존재하지 않으면 메모리에 있는 페이지를 스왑아웃하여 빈 공간을 확보해야 한다고 했다. 이것을 **페이지 교체**(page replacement)라고 한다. 이때 어떤 페이지를 쫓아낼 것인지 결정하는 알고리즘을 교체 알고리즘(replacement algorithm)이라고 한다. 이 알고리즘의 목표는 페이지 부재율을 최소화하는 것이다.

<br>

### 1. 최적 페이지 교체

페이지 부재율을 최소화하기 위해 **페이지 중 가장 먼 미래에 참조될 페이지를 쫓아내면 된다.** 이러한 알고리즘을 빌레디의 최적 알고리즘(Belady's optimal algorithm) 또는 MIN, OPT 등의 이름으로 부른다.

![image](https://user-images.githubusercontent.com/62419307/116117670-77161180-a6f7-11eb-96e3-95443c43de61.png)

초기 4회는 불가피하게 페이지 부재가 발생하고 페이지 5를 참조할 때는 페이지 교체가 필요한데 그때 사용하는 게 최적 페이지라면 4번이 스왑아웃된다.

+ 어떠한 순서로 참조될지 미리 알아야 하기 때문에 실제 온라인으로 사용할 수 있는 알고리즘은 아니다.
+ 페이지 교체 알고리즘 중 가장 부재율이 적기 때문에 다른 알고리즘의 성능에 대한 상한선을 제공한다.

<br>

### 2. 선입선출 알고리즘

선입선출 알고리즘(First In First Out : FIFO) 알고리즘은 페이지 교체 시 **물리적 메모리에 가장 먼저 올라온 페이지를 우선적으로 내쫓는다.** 무조건 먼저 들어온 페이지를 교체하기 때문에 가장 먼저 들어온 페이지가 많은 참조가 이루어진다고 해도 내쫓게 된다는 비효율적인 상황이 발생할 수 있다.

![image](https://user-images.githubusercontent.com/62419307/116118330-20f59e00-a6f8-11eb-80f2-c7ad1e852aa2.png)

그림에서 보는 것처럼 FIFO 알고리즘에서 메모리를 증가시켰음에도 불구하고 페이지 부재가 오히려 늘어났다. 이런 상황을 **FIFO의 이상 현상**(FIFO anomaly)라고 한다.

<br>

### 3. LRU 알고리즘

미래를 알아내기 위해서는 과거를 보면 된다. 페이지 참조 성향 중 **시간 지역성**(temporal locality)라는 게 있는데 이 성질은 최근에 참조된 페이지가 가까운 미래에 다시 참조될 가능성이 높다는 것을 의미한다. LRU(Least Recently Used) 알고리즘은 이와 같은 성질을 이용하여 **페이지 교체 시 가장 오래 전에 참조가 이루어진 페이지를 쫓아낸다.**

![image](https://user-images.githubusercontent.com/62419307/116119035-f0623400-a6f8-11eb-9995-9ed99fd4c762.png)

 페이지 5를 참조할 때는 페이지 교체가 필요한데 페이지 3이 가장 오래 전에 참조된 페이지이기 때문에 페이지 3과 교체된다.

<br>

### 4. LFU 알고리즘

LFU(Least Frequently Used) 알고리즘은 **페이지의 참조 횟수로 교체시킬 페이지를 결정**한다. 즉, 페이지 중에서 과거에 참조 횟수(reference count)가 가장 적었던 페이지를 쫓아낸다. 만약 최저 참조 횟수를 가진 페이지가 여러 개라면 하나를 임의로 선정한다.

성능 향상을 위해서 최저 참조 횟수를 가진 페이지들 중 상대적으로 더 오래 전에 참조된 페이지를 교체하도록 구현할 수 있다.

LFU 알고리즘은 LRU 알고리즘보다 오랜 시간 동안의 참조 기록을 반영할 수 있다는 장점이 있다. 왜냐면 LRU의 경우 직저에 참조된 시점만을 반영하지만 LFU는 보다 장기적인 관점에서 참조 횟수를 고려하기 때문이다.

![image](https://user-images.githubusercontent.com/62419307/116120660-be51d180-a6fa-11eb-9007-0cc56f584dc1.png)

LRU 알고리즘의 경우 최근 참조 여부만 보기 때문에 참조 횟수가 많은 인기 있는 페이지여도 교체한다는 단점이 있고, LFU 알고리즘도 지금부터 인기를 얻는 페이지일 수 있지만 참조 횟수가 가장 적은 페이지이기 때문에 교체된다는 단점이 있다.

<br>

![image](https://user-images.githubusercontent.com/62419307/116121955-205f0680-a6fc-11eb-8e0a-6569d952247c.png)

LRU 알고리즘의 경우 Linked List로 구현할 수 있다. 가장 최근에 참조된 페이지가 아래, 가장 오래 전에 참조된 페이지가 윗부분이다.

즉, 새로 참조되는 페이지는 MRU 페이지 뒤로 붙을 수 있다. 페이지가 재참조될 때는 어떻게 움직일까? 😏

그림에서 2번째 프로세스가 최근에 참조되었다고 가정하자. 그러면 그 페이지를 가장 아랫 부분에 이으면 된다. 

페이지 교체 시점에는 제일 윗 부분을 빼주면 된다.

이때의 알고리즘 성능은 O(1)의 시간 복잡도를 가진다.

<br>

![image](https://user-images.githubusercontent.com/62419307/116122150-5e5c2a80-a6fc-11eb-873d-1fb5a9cb317b.png)

LFU 알고리즘도 Linked List로 구현해 보자. 똑같이 참조횟수가 가장 많은 페이지는 아래, 가장 적은 페이지는 윗 부분이다.

페이지 교체 시점에는 어떻게 작동할까?

그림에서 2번째 프로세스가 최근에 참조되었다고 가정하자. 그러면 LRU처럼 가장 아래에 이으면 될까? 🙄

그렇지 않다. '최근 참조 여부'만 확인했던 LRU와 달리 참조 횟수를 비교해야 하는 LFU는 나보다 참조횟수가 많은 페이지를 찾기 위해서 비교 연산이 필요하다. 최악의 경우 페이지 수인 n만큼의 시간 복잡도를 가진다.

그러면 어떻게 구현할 수 있을까? 🔎

![image](https://user-images.githubusercontent.com/62419307/116122510-d75b8200-a6fc-11eb-8d0d-78c034582f66.png)

힙으로 구성하면 된다! 그러면 자식 노드들 즉, 2개의 노드들만 비교하는 연산이 이루어질 것이기 때문에 O(logn)의 시간 복잡도를 가지게 된다.