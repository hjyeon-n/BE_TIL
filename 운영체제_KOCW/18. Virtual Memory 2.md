# Virtual Memory 2

### 다양한 캐싱 환경

#### 캐싱 기법

+ 한정된 빠른 공간(=캐시)에 요청된 데이터를 저장해 두었다가 후속 요청 시 캐시로부터 직접 서비스하는 방식
+ paging system 외에도 cache memory, buffer caching, Web caching 등 다영한 분야에서 사용

<br>

#### 캐시 운영의 시간 제약

+ 교체 알고리즘에서 삭제할 항목을 결정하는 일에 지나치게 많은 시간이 걸리는 경우 실제 시스템이서 사용할 수 없음
+ Buffer caching이나 Web caching의 경우
  + O(1)에서 O(log n) 정도까지 허용
+ Paging system인 경우
  + page fault인 경우에만 OS가 관여함
  + 페이지가 이미 메모리에 존재하는 경우 참조시각 등의 정보를 OS가 알 수 없음
  + O(1)인 LRU의 list 조작조차 불가능

<br>

#### 근데 왜 갑자기 캐싱 기법이 왜 나와? 🙄

지난 시간에 배웠던 LRU 기법과 LFU 기법에 대해서 다시 생각해 보자!

CPU는 논리 주소를 물리 주소로 바꾸기 위해서 MMU를 사용한다. 예전에도 언급했듯이 이 과정은 **운영체제의 개입이 필요 없다.** 페이지 테이블에서 논리 주소에 대한 프레임의 유효-무효 비트가 valid라면 페이지 폴트가 발생하지 않았기 때문에 그 값을 그냥 읽어오기만 하면 된다.

하지만 페이지 폴트가 발생했다면 어떻게 될까? 그러면 디스크의 swap 영역에 접근해야 한다. 이 작업은 커널이 수행해야 할 I/O 작업이다. 그래서 trap이 발생하고 제어권이 커널에게 넘어간다.

이 과정 중 필요하다면 페이지 교체가 발생한다. **운영체제가 페이지를 이제 교체하려고 하는데 LRU, LFU로 페이지 교체가 가능할까? 👻** 즉, 운영체제는 참조가 가장 오래된 페이지나 참조 횟수가 제일 적은 페이지를 알 수 있을까?

정답은 '아니다.' 😲 페이지 폴트가 발생하지 않는다면 운영체제는 개입을 하지 않기 때문에 운영체제가 알 수 있는 정보는 페이지 폴트 발생 시 새로운 페이지를 교체했을 때의 시간만 알 수 있다.

<br>

지난 시간에 배운 LRU와 LFU 알고리즘 구현을 생각해보자 👀

![image](https://user-images.githubusercontent.com/62419307/116441022-05bb9780-a88c-11eb-9e15-ec547118a5b3.png)

LRU 구현 방법은 Linked List다. 운영체제가 페이지를 교체한 후 그 페이지가 가장 최근에 참조된 페이지이기 때문에 아래에 이을 수 있다. 하지만 그 list에 있는 페이지 중 재참조의 경우는 어떨까? 이때는 폴트가 발생하지 않았기 때문에 운영체제에게 제어권이 넘어가지 않는다. 

그러면 운영체제는 저 list 자료구조를 유지해서 재참조일 때는 해당 페이지를 끊어서 가장 아래로 이을 수 있을까? 그렇지 않다. 그러면 이 작업을 누가 해야 할까? 역설적이게도 운영체제이다.

<br>

#### 아니 뭔 소리야... 😵

그렇기 때문에 사실 가상 메모리 환경에서 LRU, LFU 알고리즘을 사용하지 못한다...! 대신에 buffer chaching이나 web caching에서 사용이 가능하다.

<br>

#### 그러면 페이징 시스템에서는 뭘 쓰는 거야? 🤯

그래서 이제 배울 Clock Algorithm을 사용한대! 

<br>

### 클럭 알고리즘

**클럭 알고리즘**(clock algorithm)은 하드웨어 지원을 통해 LRU와 LFU 알고리즘의 운영 오버헤드를 줄인 방식이다.

클럭 알고리즘은 LRU를 근사시킨 알고리즘으로 **NUR**(Not Used Recently) 또는 **NRU**(Not Recently Used) 알고리즘이라고도 한다.

LRU 알고리즘이 가장 오래 전에 참조된 페이지를 교체하는 것에 비해 클럭 알고리즘은 **오랫동안 참조되지 않은 페이지 중 하나를 교체**한다. 앞서 말했듯이 하드웨어적인 지원으로 동작하기 때문에 LRU에 비해 페이지의 관리가 훨씬 빠르고 효율적으로 이루어진다.

따라서 대부분의 시스템에서 페이지 교체 알고리즘으로 클럭 알고리즘을 선택한다.

<br>

클럭 알고리즘은 말그대로 시계와 같은 모양이다! ⏲

교체할 페이지를 선정하기 위해서 페이지 프레임들의 참조비트를 순차적으로 조사한다. 프레임 내의 페이지가 참조되면 하드웨어에 의해 1로 자동 세팅된다. 여기서 1인 페이지는 0으로 바꾸고 그냥 넘어가고 0인 페이지는 교체한다. 즉, **오랫동안 참조되지 않은 페이지 중 하나를 교체**한다는 의미가 이런 것이다! 그래서 클럭 알고리즘을 2차 기회 알고리즘(second chance algorithm)이라고 한다.

![image](https://user-images.githubusercontent.com/62419307/116442963-11a85900-a88e-11eb-8f1f-7395706ffbdf.png)

modified bit(dirty bit)도 있는데 이 비트가 1일 경우 최근에 변경된 페이지 즉, I/O를 동반하는 페이지를 의미한다. 이 페이지를 스왑 아웃할 때는 백킹 스토어에 수정사항을 저장한다.

<br>

### 페이지 프레임의 할당

프로세스가 여러 개가 동시에 수행되는 상황에서 각 프로세스에 얼마만큼의 메모리 공간을 할당할 것인지 결정해야 한다.

기본적인 할당 알고리즘은 세 가지로 나눌 수 있다.

+ **균등할당**(eqaul allocation) : 모든 프로세스에게 페이지 프레임을 균일하게 할당
+ **비례할당**(proportional allocation) : 프로세스의 크기에 비례해 페이지 프레임을 할당 
+ **우선순위 할당**(priority allocation) : 프로세스의 우선순위에 따라 페이지 프레임을 다르게 할당

하지만 이 방법이 항상 정답은 아니다. 현재 수행 중인 프로세스의 수가 너무 많아지면 프로세스에 할당할 수 있는 메모리 양이 적어지기 때문이다. 또한, 반복문을 실행 중인 프로세스인 경우 반복문을 구성하는 페이지 수보다 적은 양의 프레임을 할당 받는다면 반복문을 돌 때마다 페이지 폴트가 발생할 수 있기 때문에 주의해야 한다. 따라서 종합적인 상황을 고려해야 한다.

<br>

### 전역교체와 지역교체

+ 전역 교체 : 모든 페이지 프레임이 교체 대상이 될 수 있음. 다른 프로세스에 할당된 프레임을 빼앗아 올 수 있다. ex) LRU, LFU, Clock
+ 지역 교체 : 현재 수행 중인 프로세스에게 할당된 프레임 내에서만 교체 대상을 선정할 수 있는 방법. 즉, 프로세스마다 페이지 프레임을 미리 할당한다.

<br>

### 스레싱

일정 수준 이상의 프레임을 할당 받아야 프로세스가 원활하게 수행될 수 있다. 만약 집중적으로 참조되는 페이지들의 집합을 메모리에 한꺼번에 적재되지 않는다면 페이지 부재율이 크게 상승해서 CPU 이용률이 떨어질 수 있다. 이런 상황을 **스레싱**(thrashing)이라고 한다. 좀 더 자세히 알아보자!

<br>

운영체제는 CPU 이용률이 낮으면 이렇게 생각한다. "음... 메모리에 올라와 있는 프로세스의 수가 적군... 😒" 따라서 CPU에게 계속 일을 시키기 위해서 운영체제는 메모리에 올라가는 프로세스의 수를 늘리게 된다. 메모리에 동시에 올라가 있는 프로세스의 수를 **다중 프로그래밍의 정도**(Multi-Programming Degree : MPD)라고 한다. 결국 운영체제는 MPD를 높이게 되는 것이다.

하지만 MPD가 너무 높아지면 당연히 프로세스에 할당되는 메모리 양이 감소한다! 😱 그 결과로 페이지 부재율이 상승한다.

페이지 부재율이 상승해 디스크에 접근해야 하고 이 일은 운영체제가 해야 하기 때문에 준비 큐에 있는 프로세스의 수가 적어진다. 그러면 운영체제는 뭐라고 생각한다고? "음... 메모리에 올라와 있는 프로세스의 수가 적군... 🧐" 다시 말해 이 상황이 바로 스래싱이다.

![image](https://user-images.githubusercontent.com/62419307/116445760-305c1f00-a891-11eb-88d3-9daac3f0b023.png)

이 MPD를 조절해 CPU 이용률을 높이고 페이지 부재율을 낮히는 방법엔 **워킹셋 알고리즘**(working-set algorithm)과 **페이지 부재 빈도 알고리즘**(page-fault frequency scheme)이 있다.

<br>

#### 1. 워킹셋 알고리즘

프로세스는 일정 시간 동안 특정 주소 영역을 집중적으로 참조하는 경향이 있는데 이때 참조되는 페이지들의 집합을 locality set이라고 한다. 이와 같이 워킹셋 알고리즘은 지역성 집합이 메모리에 동시에 올라갈 수 있도록 보장하는 메모리 관리 알고리즘을 의미한다.

그러면 어떻게 작동할까?

워킹셋(working-set)은 지역성 특징 때문에 한꺼번에 메모리에 올라와 있어야 하는 페이지들의 집합을 말한다. 그리고 이 워킹셋이 다 올라갈 수 있을 때만 그 프로세스에게 메모리를 할당한다. 그렇지 않으면 할당된 페이지 프레임을 모두 스왑아웃한다. (와 화끈해... 🔥)

<br>

#### 2. 페이지 부재 빈도 알고리즘

페이지 부재 빈도 알고리즘(Page Fault Frequency: PFF) 알고리즘은 프로세스의 페이지 부재율을 주기적으로 조사하고 이 값에 근거해서 각 프로세스에 할당할 메모리 양을 동적으로 조절한다. 즉, 상한값이나 하한값을 넘어가면 프레임을 할당하거나 반납한다.